version: v0.6
superbench:
  enable:
  - ib-traffic:allpairs
  - nccl-bw:allpairs
  - model-benchmarks:stress
  monitor:
    enable: true
    sample_duration: 1
    sample_interval: 10
  var:
    nccl_env: &nccl_env
      NCCL_IB_PCI_RELAXED_ORDERING: '1'
      NCCL_NET_GDR_LEVEL: '5'
      NCCL_TOPO_FILE: /opt/microsoft/ndv4-topo.xml
      NCCL_DEBUG: WARN
    nccl_allreduce_config: &nccl_allreduce_config
      timeout: 1200
      modes:
      - name: mpi
        proc_num: 8
        node_num: all
        mca:
          routed: direct
        pattern:
          type: pair-wise
        env:
          <<: *nccl_env
      parameters:
        # run_count: 5
        minbytes: 1K
        maxbytes: 16G
        stepfactor: 2
        check: 1
        warmup_iters: 20
        iters: 100
    torch_dist_config: &torch_dist_config
      timeout: 3600
      modes:
      - name: torch.distributed
        proc_num: 8
        node_num: all
        env:
          <<: *nccl_env
      frameworks: [pytorch]
      models: [gpt2-large]
      parameters:
        # run_count: 5
        duration: 1800
        num_warmup: 64
        num_steps: -100
        sample_count: 8192
        batch_size: 8
        seq_len: 224
        precision: [float32]
        model_action: [train]
        pin_memory: yes
  benchmarks:
    ib-traffic:allpairs:
      modes:
      - name: mpi
        proc_num: 8
        node_num: all
        mca:
          routed: direct
      parameters:
        msg_size: 8388608
        bidirectional: yes
        ib_dev: mlx5_ib$LOCAL_RANK
        gpu_dev: $LOCAL_RANK
        numa_dev: $((LOCAL_RANK/2))
    nccl-bw:allpairs:
      <<: *nccl_allreduce_config
    # model benchmark - training
    model-benchmarks:stress:
      <<: *torch_dist_config
